import torch
import torch.nn as nn
import numpy as np
import pickle
import sqlite3
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from timeit import default_timer as timer
from typing import Tuple

from utils import (
    tokenize_sequences, rna2vec, seq2vec, rna2vec_pretraining,
    get_dataset, get_scores, argument_seqset, API_Dataset, Masked_Dataset
)
from encoders import Token_Predictor, Encoders, CONVBlocks, Predictor, To_IteractionMap, AptaTransWrapper
from mcts import MCTS


class Timer:
    """Class for timing blocks of code."""
    def __init__(self, name: str):
        self.name = name

    def __enter__(self):
        self.start = timer()

    def __exit__(self, exc_type, exc_value, exc_traceback):
        print(f"{self.name}: {timer() - self.start:.4f} seconds")


class AptaTransPipeline:
    def __init__(
            self,
            dim: int = 128,
            mult_ff: int = 2,
            n_layers: int = 6,
            n_heads: int = 8,
            dropout: float = 0.1,
            channel_size: int = 64,
            load_best_pt: bool = True,
            device: str = 'cpu',
            seed: int = 1004
    ):
        """Initialize AptaTransPipeline with model configurations."""
        self.seed = seed
        self.device = device
        self._initialize_constants()
        self._load_protein_words()
        self._initialize_encoders(dim, mult_ff, n_layers, n_heads, dropout, channel_size, load_best_pt)

    def _initialize_constants(self):
        """Initialize constants used in the pipeline."""
        self.n_apta_vocabs = 127
        self.n_apta_target_vocabs = 344
        self.n_prot_vocabs = 715
        self.n_prot_target_vocabs = 585
        self.apta_max_len = 275
        self.prot_max_len = 867

    def _load_protein_words(self):
        """Load protein words from a pickle file."""
        with open('./data/protein_word_freq_3.pickle', 'rb') as fr:
            words = pickle.load(fr)
            words = words[words["freq"] > words.freq.mean()].seq.values
            self.prot_words = {word: i + 1 for i, word in enumerate(words)}

    def _initialize_encoders(self, dim, mult_ff, n_layers, n_heads, dropout, channel_size=64, load_best_pt=False):
        """Initialize the encoder models and other components."""
        self.encoder_aptamer = Encoders(
            n_vocabs=self.n_apta_vocabs,
            n_layers=n_layers,
            n_heads=n_heads,
            dim=dim,
            mult_ff=mult_ff,
            dropout=dropout,
            max_len=self.apta_max_len
        ).to(self.device)
        
        self.token_predictor_aptamer = Token_Predictor(
            n_vocabs=self.n_apta_vocabs,
            n_target_vocabs=self.n_apta_target_vocabs,
            dim=dim
        ).to(self.device)
        
        self.encoder_protein = Encoders(
            n_vocabs=self.n_prot_vocabs,
            n_layers=n_layers,
            n_heads=n_heads,
            dim=dim,
            mult_ff=mult_ff,
            dropout=dropout,
            max_len=self.prot_max_len
        ).to(self.device)
        
        self.token_predictor_protein = Token_Predictor(
            n_vocabs=self.n_prot_vocabs,
            n_target_vocabs=self.n_prot_target_vocabs,
            dim=dim
        ).to(self.device)
        
        self.to_im = To_IteractionMap().to(self.device)
        self.conv = CONVBlocks(out_channels=channel_size).to(self.device)
        self.predictor = Predictor(channel_size=channel_size).to(self.device)

        if load_best_pt:
            self._load_pretrained_models()

    def _load_pretrained_models(self):
        """Load pre-trained models for aptamer and protein encoders."""
        try:
            self.encoder_aptamer.load_state_dict(torch.load("./models/encoder_apta_pretrained.pt"))
            self.encoder_protein.load_state_dict(torch.load("./models/encoder_prot_pretrained.pt"))
            print('Pre-trained models loaded successfully!')
        except FileNotFoundError:
            print('No pre-trained model files found. Pre-training required!')

    def set_data_for_training(self, batch_size: int):
        """Set data for training."""
        datapath = "./data/dataset_li.pickle"
        ds_train, ds_test = get_dataset(datapath, self.prot_max_len, self.n_prot_vocabs, self.prot_words)
        self.train_loader = DataLoader(API_Dataset(ds_train[0], ds_train[1], ds_train[2]), batch_size=batch_size, shuffle=True)
        self.test_loader = DataLoader(API_Dataset(ds_test[0], ds_test[1], ds_test[2]), batch_size=batch_size, shuffle=False)

    def set_data_rna_pt(self, batch_size: int, masked_rate: float = 0.15):
        """Set data for RNA pre-training."""
        conn = sqlite3.connect("./data/bpRNA.db")
        seqset = self._fetch_rna_sequences(conn)
        seqset = argument_seqset(seqset)
        train_seq, test_seq = train_test_split(seqset, test_size=0.05, random_state=self.seed)
        train_x, train_y = rna2vec_pretraining(train_seq)
        test_x, test_y = rna2vec_pretraining(test_seq)
        self.rna_train = self._create_dataloader(train_x, train_y, batch_size, masked_rate, self.apta_max_len, isrna=True)
        self.rna_test = self._create_dataloader(test_x, test_y, batch_size, masked_rate, self.apta_max_len, isrna=True)

    def _fetch_rna_sequences(self, conn):
        """Fetch RNA sequences from the database."""
        results = conn.execute("SELECT * FROM RNA")
        fetch = results.fetchall()
        return [[f[1], f[2]] for f in fetch if len(f[1]) <= 277]

    def _create_dataloader(self, x, y, batch_size, masked_rate, max_len, isrna=False):
        """Create a DataLoader for masked datasets."""
        dataset = Masked_Dataset(x, y, max_len, masked_rate, self.n_apta_vocabs - 1 if isrna else self.n_prot_vocabs - 1, isrna=isrna)
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)

    def set_data_protein_pt(self, batch_size: int, masked_rate: float = 0.15):
        """Set data for protein pre-training."""
        conn = sqlite3.connect("./data/protein_ss_keywords.db")
        seqset = self._fetch_protein_sequences(conn)
        train_seq, test_seq = train_test_split(seqset, test_size=0.05, random_state=self.seed)
        train_x, train_y = seq2vec(train_seq, self.prot_max_len, self.n_prot_vocabs, self.n_prot_target_vocabs, self.prot_words, self._get_protein_words_ss())
        test_x, test_y = seq2vec(test_seq, self.prot_max_len, self.n_prot_vocabs, self.n_prot_target_vocabs, self.prot_words, self._get_protein_words_ss())
        self.protein_train = self._create_dataloader(train_x, train_y, batch_size, masked_rate, self.prot_max_len)
        self.protein_test = self._create_dataloader(test_x, test_y, batch_size, masked_rate, self.prot_max_len)

    def _fetch_protein_sequences(self, conn):
        """Fetch protein sequences from the database."""
        results = conn.execute("SELECT SEQUENCE, SS FROM PROTEIN")
        fetch = results.fetchall()
        return [[f[0], f[1]] for f in fetch]

    def _get_protein_words_ss(self):
        """Get protein words for secondary structure prediction."""
        ss = ['', 'H', 'B', 'E', 'G', 'I', 'T', 'S', '-']
        words_ss = np.array([i + j + k for i in ss for j in ss for k in ss[1:]])
        words_ss = np.unique(words_ss)
        return {word: i + 1 for i, word in enumerate(words_ss)}

    def train(self, epochs: int, lr: float = 1e-5):
        """Train the model."""
        print('Training the model!')
        best_auc = 0
        self._initialize_optimizer_and_criterion(lr)

        for epoch in range(1, epochs + 1):
            self._train_epoch(epoch, best_auc)

    def _initialize_optimizer_and_criterion(self, lr):
        """Initialize optimizer and criterion for training."""
        model_parameters = (
            list(self.encoder_aptamer.parameters()) +
            list(self.encoder_protein.parameters()) +
            list(self.to_im.parameters()) +
            list(self.conv.parameters()) +
            list(self.predictor.parameters())
        )
        self.optimizer = torch.optim.AdamW(model_parameters, lr=lr, weight_decay=1e-5)
        self.criterion = nn.BCELoss().to(self.device)

    def _set_train_mode(self):
        """Set the model to train mode."""
        self.encoder_aptamer.train()
        self.encoder_protein.train()
        self.to_im.train()
        self.conv.train()
        self.predictor.train()

    def _set_eval_mode(self):
        """Set the model to evaluation mode."""
        self.encoder_aptamer.eval()
        self.encoder_protein.eval()
        self.to_im.eval()
        self.conv.eval()
        self.predictor.eval()

    def _train_epoch(self, epoch, best_auc):
        """Train the model for a single epoch."""
        self._set_train_mode()
        loss_train, pred_train, target_train = self._batch_step(self.train_loader, train_mode=True)
        print(f"\n[EPOCH: {epoch}], \tTrain Loss: {loss_train:.6f}", end='')

        self._set_eval_mode()
        with torch.no_grad():
            loss_test, pred_test, target_test = self._batch_step(self.test_loader, train_mode=False)
            scores = get_scores(target_test, pred_test)
            print(f"\tTest Loss: {loss_test:.6f}\tTest ACC: {scores['acc']:.6f}\tTest AUC: {scores['roc_auc']:.6f}\t"
                  f"Test MCC: {scores['mcc']:.6f}\tTest PR_AUC: {scores['pr_auc']:.6f}\tF1: {scores['f1']:.6f}\n")

        if scores['roc_auc'] > best_auc:
            best_auc = scores['roc_auc']
            self._save_best_model()

    def _batch_step(self, loader: DataLoader, train_mode: bool = True) -> Tuple[float, np.ndarray, np.ndarray]:
        """Perform a single batch step."""
        loss_total = 0
        pred, target = [], []

        for batch_idx, (apta, prot, y) in enumerate(loader):
            if train_mode:
                self.optimizer.zero_grad()

            y_pred = self.predict(apta, prot)
            y_true = y.clone().detach().float().to(self.device)
            loss = self.criterion(torch.flatten(y_pred), y_true)

            if train_mode:
                loss.backward()
                self.optimizer.step()

            loss_total += loss.item()
            pred.extend(torch.flatten(y_pred).clone().detach().cpu().numpy())
            target.extend(torch.flatten(y_true).clone().detach().cpu().numpy())
            self._print_batch_progress(batch_idx, loader, train_mode, loss_total)

        return loss_total / len(loader), np.array(pred), np.array(target)

    def predict(self, apta: torch.Tensor, prot: torch.Tensor) -> torch.Tensor:
        """Predict the output using the model."""
        apta, prot = apta.to(self.device), prot.to(self.device)
        apta_encoded = self.encoder_aptamer(apta)
        prot_encoded = self.encoder_protein(prot)
        interaction_map = self.to_im(apta_encoded, prot_encoded)
        out = self.conv(interaction_map)
        out = self.predictor(out)
        return out

    def generate_interaction_map(self, apta, prot):
        """Generate interaction map for the given aptamer-protein pair."""
        apta, prot = apta.to(self.device), prot.to(self.device)
        apta_encoded = self.encoder_aptamer(apta)
        prot_encoded = self.encoder_protein(prot)
        interaction_map = self.to_im(apta_encoded, prot_encoded)
        return interaction_map

    def predict_proba(self, interaction_map):
        """Predict the probability of interaction."""
        out = torch.tensor(interaction_map).float().to(self.device)
        out = torch.unsqueeze(out, 1)
        out = torch.mean(out, 4)
        out = self.conv(out)
        out = self.predictor(out)
        out = np.array([[1 - o[0], o[0]] for o in out.clone().detach().cpu().numpy()])
        return out

    def pretrain_encoder_aptamer(self, epochs: int, lr: float = 1e-4, weight_decay: float = 1e-5):
        """Pre-train the aptamer encoder."""
        savepath = "./models/rna_pretrained_encoder.pt"
        model_parameters = list(self.encoder_aptamer.parameters()) + list(self.token_predictor_aptamer.parameters())
        self.optimizer_pt_apta = torch.optim.AdamW(model_parameters, lr=lr, weight_decay=weight_decay)
        self.criterion_mlm_apta = nn.CrossEntropyLoss().to(self.device)
        self.criterion_ssp_apta = nn.CrossEntropyLoss().to(self.device)
        best_loss = float('inf')

        with Timer("Total training time"):
            for epoch in range(1, epochs + 1):
                self.encoder_aptamer.train()
                self.token_predictor_aptamer.train()
                train_loss, train_mlm, train_ssp = self._run_epoch_pt_apta(self.rna_train, train_mode=True)
                print(f"\n[EPOCH {epoch}] Train Loss: {train_loss:.6f} Train MLM: {train_mlm:.6f} Train SSP: {train_ssp:.6f}", end='')

                with torch.no_grad():
                    self.encoder_aptamer.eval()
                    self.token_predictor_aptamer.eval()
                    test_loss, test_mlm, test_ssp = self._run_epoch_pt_apta(self.rna_test, train_mode=False)
                    print(f" Test Loss: {test_loss:.6f} Test MLM: {test_mlm:.6f} Test SSP: {test_ssp:.6f}")

                    if test_loss < best_loss:
                        best_loss = test_loss
                        torch.save(self.encoder_aptamer.state_dict(), savepath)

    def _run_epoch_pt_apta(self, loader: DataLoader, train_mode: bool) -> Tuple[float, float, float]:
        """Run a single epoch for pre-training."""
        total_loss, total_mlm, total_ssp = 0.0, 0.0, 0.0

        for batch_idx, (x_masked, y_masked, x, y_ss) in enumerate(loader):
            inputs_mlm, inputs, y_masked, y_ss = self._to_device(x_masked, x, y_masked, y_ss)
            y_masked = y_masked.long()
            y_ss = y_ss.long()

            mlm_apta = self.encoder_aptamer(inputs_mlm)
            ssp_apta = self.encoder_aptamer(inputs)
            y_pred_mlm, y_pred_ssp = self.token_predictor_aptamer(mlm_apta, ssp_apta)

            l_mlm = self.criterion_mlm_apta(y_pred_mlm.transpose(1, 2), y_masked)
            l_ssp = self.criterion_ssp_apta(y_pred_ssp.transpose(1, 2), y_ss)
            loss = l_mlm * 2 + l_ssp

            if torch.isnan(loss):
                print(f"NaN detected in loss at batch {batch_idx} [MLM: {l_mlm.item()}][SSP: {l_ssp.item()}]")
                self.debug_forward_pass(inputs, inputs_mlm, y_pred_mlm, y_pred_ssp, l_mlm, l_ssp, loss)
                continue

            if train_mode:
                self.optimizer_pt_apta.zero_grad()
                loss.backward()
                self.optimizer_pt_apta.step()

            total_mlm += l_mlm.item()
            total_ssp += l_ssp.item()
            total_loss += loss.item()
            self._print_batch_progress(batch_idx, loader, train_mode, total_loss)
        return total_loss / len(loader), total_mlm / len(loader), total_ssp / len(loader)

    def pretrain_encoder_protein(self, epochs: int, lr: float = 1e-4, weight_decay: float = 1e-5):
        """Pre-train the protein encoder."""
        savepath = "./models/protein_pretrained_encoder.pt"
        model_parameters = list(self.encoder_protein.parameters()) + list(self.token_predictor_protein.parameters())
        self.optimizer_pt_prot = torch.optim.AdamW(model_parameters, lr=lr, weight_decay=weight_decay)
        self.criterion_mlm_prot = nn.CrossEntropyLoss().to(self.device)
        self.criterion_ssp_prot = nn.CrossEntropyLoss().to(self.device)
        best_loss = float('inf')

        with Timer("Total training time"):
            for epoch in range(1, epochs + 1):
                train_loss, train_mlm, train_ssp = self._run_epoch_pt_prot(self.protein_train, train_mode=True)
                print(f"\n[EPOCH {epoch}] Train Loss: {train_loss:.6f} Train MLM: {train_mlm:.6f} Train SSP: {train_ssp:.6f}", end='')

                test_loss, test_mlm, test_ssp = self._run_epoch_pt_prot(self.protein_test, train_mode=False)
                print(f" Test Loss: {test_loss:.6f} Test MLM: {test_mlm:.6f} Test SSP: {test_ssp:.6f}")

                if test_loss < best_loss:
                    best_loss = test_loss
                    torch.save(self.encoder_protein.state_dict(), savepath)

    def _run_epoch_pt_prot(self, loader: DataLoader, train_mode: bool) -> Tuple[float, float, float]:
        """Run a single epoch for pre-training."""
        total_loss, total_mlm, total_ssp = 0.0, 0.0, 0.0

        for batch_idx, (x_masked, y_masked, x, y_ss) in enumerate(loader):
            inputs_mlm, inputs, y_masked, y_ss = self._to_device(x_masked, x, y_masked, y_ss)
            y_masked = y_masked.long()
            y_ss = y_ss.long()

            mlm_prot = self.encoder_protein(inputs_mlm)
            ssp_prot = self.encoder_protein(inputs)
            y_pred_mlm, y_pred_ssp = self.token_predictor_protein(mlm_prot, ssp_prot)

            l_mlm = self.criterion_mlm_prot(y_pred_mlm.transpose(1, 2), y_masked)
            l_ssp = self.criterion_ssp_prot(y_pred_ssp.transpose(1, 2), y_ss)
            loss = l_mlm * 2 + l_ssp

            if torch.isnan(loss):
                print(f"NaN detected in loss at batch {batch_idx} [MLM: {l_mlm.item()}][SSP: {l_ssp.item()}]")
                self.debug_forward_pass(inputs, inputs_mlm, y_pred_mlm, y_pred_ssp, l_mlm, l_ssp, loss)
                continue

            if train_mode:
                self.optimizer_pt_prot.zero_grad()
                loss.backward()
                self.optimizer_pt_prot.step()

            total_mlm += l_mlm.item()
            total_ssp += l_ssp.item()
            total_loss += loss.item()
            self._print_batch_progress(batch_idx, loader, train_mode, total_loss)
        return total_loss / len(loader), total_mlm / len(loader), total_ssp / len(loader)

    def _to_device(self, *tensors):
        """Move tensors to the device."""
        return [tensor.to(self.device) for tensor in tensors]

    def _has_nan_or_inf(self, *tensors):
        """Check for NaN or infinity in tensors."""
        for tensor in tensors:
            if torch.isnan(tensor).any() or torch.isinf(tensor).any():
                print(f"Invalid value detected in tensors")
                return True
        return False

    def _print_batch_progress(self, batch_idx, loader, train_mode, values=None):
        """Print progress for the current batch."""
        mode = 'train' if train_mode else 'eval'
        print(f"{mode}[{batch_idx}/{len(loader)}({100. * batch_idx / len(loader):.0f}%)][Loss: {values:.4f}]    ", end='\r', flush=True)

    def debug_forward_pass(self, inputs, inputs_mlm, y_pred_mlm, y_pred_ssp, l_mlm, l_ssp, loss):
        """Print debug information for the forward pass."""
        print(f"Inputs MLM: {inputs_mlm}")
        print(f"Inputs: {inputs}")
        print(f"Y Pred MLM: {y_pred_mlm}")
        print(f"Y Pred SSP: {y_pred_ssp}")
        print(f"Loss MLM: {l_mlm}")
        print(f"Loss SSP: {l_ssp}")
        print(f"Total Loss: {loss}")

    def inference(self, apta: str, prot: str) -> np.ndarray:
        """Predict the aptamer-protein interaction."""
        print('Predicting the Aptamer-Protein Interaction')
        self._load_best_model()

        print(f'Aptamer: {apta}')
        print(f'Target Protein: {prot}')

        apta_tokenized = torch.tensor(rna2vec(np.array([apta])), dtype=torch.int64).to(self.device)
        prot_tokenized = torch.tensor(tokenize_sequences([prot], self.prot_max_len, self.n_prot_vocabs, self.prot_words), dtype=torch.int64).to(self.device)

        y_pred = self.predict(apta_tokenized, prot_tokenized)
        score = y_pred.detach().cpu().numpy()
        print(f'Score: {score}')

        return score

    def _load_best_model(self):
        """Load the best model for API."""
        try:
            print("Loading the best model for API!")
            self.encoder_aptamer.load_state_dict(torch.load('./models/encoder_apta_best_auc.pt', map_location=self.device))
            self.encoder_protein.load_state_dict(torch.load('./models/encoder_prot_best_auc.pt', map_location=self.device))
            self.to_im.load_state_dict(torch.load('./models/to_im_best_auc.pt', map_location=self.device))
            self.conv.load_state_dict(torch.load('./models/conv_best_auc.pt', map_location=self.device))
            self.predictor.load_state_dict(torch.load('./models/predictor_best_auc.pt', map_location=self.device))
        except FileNotFoundError:
            print('No best model file found. Training required!')

    def _save_best_model(self):
        """Save the best model for API."""
        try:
            torch.save(self.encoder_aptamer.state_dict(), "./models/encoder_apta_best_auc.pt")
            torch.save(self.encoder_protein.state_dict(), "./models/encoder_prot_best_auc.pt")
            torch.save(self.to_im.state_dict(), "./models/to_im_best_auc.pt")
            torch.save(self.conv.state_dict(), "./models/conv_best_auc.pt")
            torch.save(self.predictor.state_dict(), "./models/predictor_best_auc.pt")
            print('Saved the best model!')
        except Exception as e:
            print(f"Error saving the best model: {e}")

    def recommend(self, target: str, n_aptamers: int, depth: int, iteration: int, verbose: bool = True):
        """Recommend aptamers for a target protein."""
        self._load_best_model()
        self.classifier = AptaTransWrapper(
            self.encoder_aptamer, self.encoder_protein, self.to_im, self.conv, self.predictor
        ).to(self.device)
        candidates = []
        scores = []
        encoded_targetprotein = self._encode_target_protein(target)
        mcts = MCTS(encoded_targetprotein, depth=depth, iteration=iteration, states=8, target_protein=target, device=self.device)

        for _ in range(n_aptamers):
            mcts.make_candidate(self.classifier)
            candidates.append(mcts.get_candidate())
            score = self._evaluate_candidate(mcts, encoded_targetprotein, verbose)
            scores.append(score)

        self._final_evaluation(candidates, scores, encoded_targetprotein, verbose)

    def _encode_target_protein(self, target):
        """Encode the target protein."""
        return torch.tensor(tokenize_sequences([target], self.prot_max_len, self.n_prot_vocabs, self.prot_words), dtype=torch.int64).to(self.device)

    def _evaluate_candidate(self, mcts, encoded_targetprotein, verbose):
        """Evaluate a single candidate."""
        self.classifier.eval()
        with torch.no_grad():
            sim_seq = np.array([mcts.get_candidate()])
            apta = torch.tensor(rna2vec(sim_seq), dtype=torch.int64).to(self.device)
            score = self.classifier(apta, encoded_targetprotein)

        if verbose:
            print(f"Candidate: {mcts.get_candidate()}\tScore: {score}")
            print("*" * 80)
        mcts.reset()

        return score

    def _final_evaluation(self, candidates, scores, encoded_targetprotein, verbose):
        """Perform final evaluation of candidates."""
        for candidate, score in zip(candidates, scores):
            if verbose:
                print(f'Candidate: {candidate}, Score: {score}')